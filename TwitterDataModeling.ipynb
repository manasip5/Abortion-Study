{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We followed the guide about using CNNs for Sentence Classification from Chris Tran (https://chriskhanhtran.github.io/posts/cnn-sentence-classification/). Thanks to Jesica Ram√≠rez for her recommendation and guidance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import preprocessing as pp\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 30255 # Specify a seed for reproducability\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentimentnum.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words for a given tweet in the Dataset:\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "print('Max number of words for a given tweet in the Dataset:')\n",
    "max_len = df.Text.apply(lambda x: len(x.split())).max()\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing classes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "positive = df[df['sentimentnum'] == 1]\n",
    "negative = df[df['sentimentnum'] == 2]\n",
    "neutral = df[df['sentimentnum'] == 0]\n",
    "\n",
    "\n",
    "positive, rejected = train_test_split(positive,\n",
    "                                             train_size=(neutral.shape[0]/\n",
    "                                                         positive.shape[0]),random_state=0)\n",
    "negative, rejected = train_test_split(negative,\n",
    "                                             train_size=(neutral.shape[0]/\n",
    "                                                         negative.shape[0]),random_state=0)\n",
    "\n",
    "data = positive.append(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentimentclass\n",
       "negative    42680\n",
       "positive    42680\n",
       "dtype: int64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('sentimentclass').size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataclassCNN.py\n",
    "File to create a Custom Data Class and Collate Function for PyTorch.\n",
    "This file is for the CNN model.\n",
    "'''\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "\n",
    "class ProjectDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, target_col=None, text_col=None):\n",
    "\n",
    "        # Target first, then Inputs.\n",
    "        self.samples = []\n",
    "        tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "        if not target_col and not text_col:\n",
    "            targets = list(data[0])\n",
    "            inputs = list(data[1])\n",
    "            for idx in range(len(targets)):\n",
    "                text = tokenizer(inputs[idx])\n",
    "                self.samples.append([targets[idx], text])\n",
    "        else:\n",
    "            for _, row in data.iterrows():\n",
    "                text = row[text_col]\n",
    "                text = tokenizer(text)\n",
    "                target = row[target_col]\n",
    "                self.samples.append([target, text])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obj = ProjectDataset(data, 'sentimentnum', 'Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ProjectDataset at 0x1472081abc8>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary length is 46655 words\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "\n",
    "counter_words = Counter()\n",
    "for (label, text) in data_obj:\n",
    "    counter_words.update(text)\n",
    "    \n",
    "vocab_words = Vocab(counter_words)\n",
    "\n",
    "print('The vocabulary length is {} words'.format(len(vocab_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_for_cnn(batch, max_len=max_len):\n",
    "    '''\n",
    "    For each batch, develop the appropiate inputs from the models (i.e. embeddings)\n",
    "    '''\n",
    "    input_vector = []\n",
    "    labels = []\n",
    "    for i, (label, tokenized_sent) in enumerate(batch):\n",
    "        \n",
    "        #Append labels \n",
    "        labels.append(label)\n",
    "        \n",
    "        #Get tokenized sentence\n",
    "        diff =  (max_len - len(tokenized_sent))\n",
    "        tokenized_sent += ['<pad>'] * diff\n",
    "        \n",
    "       \n",
    "        assert(len(tokenized_sent) == max_len), tokenized_sent\n",
    "        \n",
    "        wordstoidx = [vocab_words.stoi[w] for w in tokenized_sent]\n",
    "        input_vector.append(wordstoidx)\n",
    "    \n",
    "    return torch.tensor(labels).to(device), torch.tensor(input_vector).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train with Validation Data and Test Data Split\n",
    "train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(\n",
    "    data['Text'], data['sentimentnum'], test_size=0.1, random_state=42)\n",
    "\n",
    "#Train and Validation Split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_val_texts, train_val_labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(train_tuple, val_tuple, test_tuple,\n",
    "                batch_size=30):\n",
    "    \"\"\"Convert train, validation and test sets into Dataloaders :)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = ProjectDataset(train_tuple)\n",
    "    train_dataloader = DataLoader(train_data, shuffle=True,\n",
    "                                  batch_size=batch_size, \n",
    "                                 collate_fn=collate_for_cnn)\n",
    "    \n",
    "    # Create DataLoader for validation data\n",
    "    valid_data = ProjectDataset(val_tuple)\n",
    "    val_dataloader = DataLoader(valid_data, shuffle=False,\n",
    "                                  batch_size=batch_size, \n",
    "                                 collate_fn=collate_for_cnn)\n",
    "\n",
    "    # Create DataLoader for test data\n",
    "    test_data = ProjectDataset(test_tuple)\n",
    "    test_dataloader = DataLoader(test_data, shuffle=False,\n",
    "                                  batch_size=batch_size, \n",
    "                                 collate_fn=collate_for_cnn)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = data_loader([train_labels, train_texts],\n",
    "                                                               [val_labels, val_texts],\n",
    "                                                               [test_labels, test_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 68])\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataloader:\n",
    "    labels, inputs = i[0], i[1]\n",
    "    print(inputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SIZES = [3,4,5]\n",
    "N_FILTERS = [100, 100,100]\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = vocab_words.stoi['<pad>']\n",
    "num_classes = 2 #Low, Medium, High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_polarization(nn.Module):\n",
    "    ''' An 1D Convulational Neural Network for Sentence Classification'''\n",
    "    \n",
    "    def __init__(self, pretrained_embedding=None, freeze_embedding=False,\n",
    "                 vocab_size=None, embed_dim=None,\n",
    "                 filter_sizes=FILTER_SIZES, num_filters=N_FILTERS,\n",
    "                 num_classes=num_classes,pad_id=PAD_IDX,\n",
    "                 dropout=DROPOUT):\n",
    "        \"\"\"\n",
    "        pretrained_embedding (torch.Tensor): Pretrained embeddings with\n",
    "                shape (vocab_size, embed_dim)\n",
    "            freeze_embedding (bool): Set to False to fine-tune pretraiend\n",
    "                vectors. Default: False\n",
    "            vocab_size (int): Need to be specified when not pretrained word\n",
    "                embeddings are not used.\n",
    "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
    "                when pretrained word embeddings are not used. Default: 300\n",
    "            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n",
    "            num_filters (int): Number of filters. Default: 100\n",
    "            n_classes (int): Number of classes. Default: 3\n",
    "            dropout (float): Dropout rate. Default: 0.5\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNN_polarization, self).__init__()\n",
    "        \n",
    "        # 1. Embedding layer\n",
    "        if pretrained_embedding is not None:\n",
    "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
    "                                                          freeze=freeze_embedding)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                          embedding_dim=self.embed_dim,\n",
    "                                          padding_idx= pad_id,\n",
    "                                          )\n",
    "        \n",
    "        # 2. Convolutional Layers (for each filter size --> n-gram)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv1d(in_channels = self.embed_dim,\n",
    "                                              out_channels = num_filters[i], \n",
    "                                              kernel_size = fs)\n",
    "                                    for i, fs in enumerate(filter_sizes)\n",
    "                                    ])\n",
    "        \n",
    "    \n",
    "        # 3. Fully-connected layer\n",
    "        self.linear = nn.Linear(in_features = np.sum(num_filters), \n",
    "                                out_features = num_classes\n",
    "                               )\n",
    "        \n",
    "        \n",
    "        # Additional Feature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x_embedded = self.embedding(inputs)\n",
    "        \n",
    "        x_embedded = x_embedded.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply CNN and ReLU\n",
    "        convs_list = [F.relu(conv(x_embedded)) for conv in self.convs]\n",
    "\n",
    "        # Max pooling.\n",
    "        pooled = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]).squeeze(2)\n",
    "                  for x_conv in convs_list]\n",
    "        \n",
    "        # Concatenate Pool list to feed the fully connected layer\n",
    "        input_fc = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        \n",
    "        # Compute probabilities\n",
    "        predictions_classes = self.linear(input_fc)\n",
    "        \n",
    "        return predictions_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_an_epoch(model, dataloader, loss_function, optimizer):\n",
    "    \n",
    "    model.train() # Sets the module in training mode.\n",
    "    total_loss = 0\n",
    "    \n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        \n",
    "        # Load batch to GPU\n",
    "        labels, inputs = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Zero out any previously calculated gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #Perform a forward pass.\n",
    "        log_probs = model(inputs)\n",
    "        \n",
    "        # Compute loss and accumulate the loss values\n",
    "        loss = loss_function(log_probs, labels.long())\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Perform a backward pass to calculate gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return total_loss/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_fn):\n",
    "    '''\n",
    "    Evaluate the model on the given data (e.g. validation data or test data).\n",
    "    '''\n",
    "\n",
    "    #As we are now using dropout, we must remember to use model.eval() \n",
    "    #to ensure the dropout is \"turned off\" while evaluating.\n",
    "    model.eval()\n",
    "    \n",
    "    total_accuracy = []\n",
    "    total_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # No gradients need to be maintained during evaluation\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            \n",
    "            # Load batch to Device\n",
    "            labels, inputs = batch[0], batch[1]\n",
    "            if USE_CUDA:\n",
    "                labels, inputs = labels.cuda(), inputs.cuda()\n",
    "                \n",
    "            # Obtain probabilities of each class per sentence\n",
    "            output = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(output, labels.long())\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            # Get the predictions\n",
    "            preds = torch.argmax(output, dim=1).flatten()\n",
    "            \n",
    "            # Calculate the accuracy rate\n",
    "            accuracy = (preds == labels).sum()/len(preds)\n",
    "            total_accuracy.append(accuracy)\n",
    "            \n",
    "\n",
    "    return np.mean(total_loss), np.mean(total_accuracy)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def train_validate(model, optimizer, train_dataloader, val_dataloader, pretrained_embedding, epochs=20):\n",
    "    \n",
    "    \"\"\"Train the CNN model.\"\"\"\n",
    "    \n",
    "    #Loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Tracking best model\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_model = None\n",
    "    \n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    print(f\"{'Epoch':^7} | {'Time(secs)':^8} | {'Train Loss':^9}| {'Val Loss':^10}| {'Val Acc':^11}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        start_time = datetime.datetime.now()\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        \n",
    "        avg_loss_train = train_an_epoch(model, train_dataloader, loss_function, optimizer)\n",
    "        \n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        \n",
    "        val_loss, val_accuracy = evaluate(model, val_dataloader, loss_function)\n",
    "\n",
    "        # Track the best model\n",
    "        if len(val_losses) == 0 or val_loss < min(val_losses):\n",
    "            best_model = type(model)(pretrained_embedding=pretrained_embedding,\n",
    "                                     vocab_size=len(vocab_words), \n",
    "                                     embed_dim=model.embed_dim,\n",
    "                                     filter_sizes=filter_sizes,\n",
    "                                     num_filters=N_FILTERS, num_classes=num_classes)\n",
    "            best_model.load_state_dict(model.state_dict())\n",
    "            if USE_CUDA:\n",
    "                best_model = best_model.cuda()\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        time_diff = (datetime.datetime.now() - start_time).seconds\n",
    "        \n",
    "        print(f\"{epoch_i + 1:^7} |  {time_diff:^8}  | {avg_loss_train:^9.2f} | {val_loss:^10.2f}| {val_accuracy:^11.2f}\")\n",
    "        print('')\n",
    "        \n",
    "    return best_model, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(df.sentimentnum.unique())\n",
    "filter_sizes = [2, 3, 6] # bi-grams, tri-grams, four-grams filters\n",
    "N_FILTERS = [100,200,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN_polarization(pretrained_embedding=None,\n",
    "                 vocab_size=len(vocab_words), embed_dim=400,\n",
    "                        freeze_embedding=False,\n",
    "                        filter_sizes=filter_sizes,\n",
    "                        num_filters=N_FILTERS,\n",
    "                        num_classes=num_classes,\n",
    "                        dropout=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), \n",
    "                                 lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  | Time(secs) | Train Loss|  Val Loss |   Val Acc  \n",
      "------------------------------------------------------------\n",
      "   1    |    791     |   0.50    |    0.32   |    0.86    \n",
      "\n",
      "   2    |    1019    |   0.31    |    0.37   |    0.85    \n",
      "\n",
      "   3    |    974     |   0.24    |    0.29   |    0.89    \n",
      "\n",
      "   4    |    990     |   0.19    |    0.30   |    0.89    \n",
      "\n",
      "   5    |    981     |   0.15    |    0.36   |    0.88    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model, val_accuracies = train_validate(cnn_model, optimizer, \n",
    "                                            train_dataloader, val_dataloader, \n",
    "                                            pretrained_embedding=None,\n",
    "                                            epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
